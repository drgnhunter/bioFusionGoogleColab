{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/drgnhunter/bioFusionGoogleColab/blob/main/biofusion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C026o5dEGxT8"
      },
      "outputs": [],
      "source": [
        "pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DaCge3ihGxT9"
      },
      "outputs": [],
      "source": [
        "pip install numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "trusted": true,
        "id": "A13rEGSCGxT9"
      },
      "outputs": [],
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nRIHkmpYGxT9"
      },
      "outputs": [],
      "source": [
        "pip install torch torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JhitLhPnGxT9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Preprocessing: resized to 224x224 as required for most CNNs\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(), # Augmentation to prevent overfitting\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Use the paths confirmed by your os.walk loop\n",
        "train_dataset = datasets.ImageFolder('chest_xray/train', transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Validation set as required by the \"Validation Approach\" criteria\n",
        "val_dataset = datasets.ImageFolder('chest_xray/val', transform=transform)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "8qgVzjP74-Vd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p0Uhrw4cGxT9"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class MedicalCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MedicalCNN, self).__init__()\n",
        "        # Soundness and originality of architecture\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            nn.Conv2d(16, 32, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2)\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(64 * 28 * 28, 128), nn.ReLU(),\n",
        "            nn.Dropout(0.5), # Prevents overfitting\n",
        "            nn.Linear(128, 1), nn.Sigmoid() # Binary output for Normal vs Pneumonia\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.classifier(self.features(x))\n",
        "\n",
        "model = MedicalCNN()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LLBeEN7nGxT-"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001) # Hyperparameter choice\n",
        "criterion = nn.BCELoss() # Loss computation\n",
        "\n",
        "for epoch in range(15):\n",
        "    model.train()\n",
        "    for images, labels in train_loader:\n",
        "        # 1. Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels.float().unsqueeze(1))\n",
        "\n",
        "        # 2. Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # 3. Optimizer update\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch+1} complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OiM5ne_aGxT-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Define the file name based on your team name as per submission conventions\n",
        "model_save_path = \"pneumoniaDetectorModel.pth\"\n",
        "\n",
        "# Save the model state_dict (weights)\n",
        "torch.save(model.state_dict(), model_save_path)\n",
        "\n",
        "print(f\"Model successfully saved to {model_save_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RoaRojhSGxT-"
      },
      "outputs": [],
      "source": [
        "pip install scikit-learn seaborn matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MTWFEcdDGxT-"
      },
      "outputs": [],
      "source": [
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Preprocessing for testing (must match training resize/normalization) [cite: 89, 161]\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Loading the test set [cite: 126]\n",
        "# Note: Use the path confirmed in your directory: 'chest_xray/test'\n",
        "test_dataset = datasets.ImageFolder('chest_xray/test', transform=test_transform)\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "print(f\"Test loader defined with {len(test_dataset)} images.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chgeWDh0GxT-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "model.eval() # Set model to evaluation mode\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad(): # Disable gradient calculation for testing\n",
        "    for images, labels in test_loader:\n",
        "        outputs = model(images)\n",
        "        # Convert probabilities to binary predictions (0 or 1)\n",
        "        preds = (outputs > 0.5).float()\n",
        "\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Calculate Accuracy\n",
        "accuracy = accuracy_score(all_labels, all_preds)\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Detailed Classification Report (Precision, Recall, F1-Score)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(all_labels, all_preds, target_names=['Normal', 'Pneumonia']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o1eZjSU6GxT-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "model.eval() # Set to evaluation mode (freezes BatchNorm and Dropout) [cite: 125, 220]\n",
        "y_true, y_pred = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        outputs = model(images)\n",
        "        predictions = (outputs > 0.5).float()\n",
        "\n",
        "        y_true.extend(labels.tolist())\n",
        "        y_pred.extend(predictions.reshape(-1).tolist())\n",
        "\n",
        "# 1. Primary & Secondary Metrics [cite: 135, 136]\n",
        "print(f\"Test Accuracy: {accuracy_score(y_true, y_pred) * 100:.2f}%\")\n",
        "print(\"\\nDetailed Report:\\n\", classification_report(y_true, y_pred, target_names=['Normal', 'Pneumonia']))\n",
        "\n",
        "# 2. Mandatory Confusion Matrix [cite: 137]\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Greens',\n",
        "            xticklabels=['Normal', 'Pneumonia'],\n",
        "            yticklabels=['Normal', 'Pneumonia'])\n",
        "plt.title('Confusion Matrix - Pneumonia Detection Model')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1p6c70jNGxT_"
      },
      "outputs": [],
      "source": [
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# 1. ADVANCED DATA AUGMENTATION (Training Set Only) [cite: 89, 208]\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    # Randomly rotate images by 15 degrees to simulate different X-ray angles\n",
        "    transforms.RandomRotation(15),\n",
        "    # Randomly zoom and crop to focus on different lung areas\n",
        "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
        "    # Randomly flip images horizontally\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    # Adjust brightness and contrast to simulate different X-ray exposures\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    # Standard normalization for medical imaging [cite: 161]\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# 2. STANDARD PREPROCESSING (Validation/Test Sets) [cite: 161]\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Loading datasets using the updated transforms [cite: 126]\n",
        "train_dataset = datasets.ImageFolder('chest_xray/train', transform=train_transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "val_dataset = datasets.ImageFolder('chest_xray/val', transform=test_transform)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "test_dataset = datasets.ImageFolder('chest_xray/test', transform=test_transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eWjNfEyeGxT_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "model.eval() # Set to evaluation mode (freezes BatchNorm and Dropout) [cite: 125, 220]\n",
        "y_true, y_pred = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        outputs = model(images)\n",
        "        predictions = (outputs > 0.5).float()\n",
        "\n",
        "        y_true.extend(labels.tolist())\n",
        "        y_pred.extend(predictions.reshape(-1).tolist())\n",
        "\n",
        "# 1. Primary & Secondary Metrics [cite: 135, 136]\n",
        "print(f\"Test Accuracy: {accuracy_score(y_true, y_pred) * 100:.2f}%\")\n",
        "print(\"\\nDetailed Report:\\n\", classification_report(y_true, y_pred, target_names=['Normal', 'Pneumonia']))\n",
        "\n",
        "# 2. Mandatory Confusion Matrix [cite: 137]\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Greens',\n",
        "            xticklabels=['Normal', 'Pneumonia'],\n",
        "            yticklabels=['Normal', 'Pneumonia'])\n",
        "plt.title('Confusion Matrix - Pneumonia Detection Model')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "id": "gWpEt_F4I8Li"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.kaggle # creating .kaggle folder where the key should be placed\n"
      ],
      "metadata": {
        "id": "-iqDYLzCKIbb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp kaggle.json ~/.kaggle/ # move the key to the folder\n"
      ],
      "metadata": {
        "id": "w6vNkp4GKJlz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd # checking the present working directory\n",
        "\n"
      ],
      "metadata": {
        "id": "4KSsbpQqKNPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod 600 ~/.kaggle/kaggle.json\n"
      ],
      "metadata": {
        "id": "6KG3IpaHKSMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets list\n",
        "\n"
      ],
      "metadata": {
        "id": "YlHHiQXCKWJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d paultimothymooney/chest-xray-pneumonia"
      ],
      "metadata": {
        "id": "cJLxUXLDKcDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q chest-xray-pneumonia.zip -d ."
      ],
      "metadata": {
        "id": "lj6OVYm1MDyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "id": "qTghK8u4MYdR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Preprocessing: resized to 224x224 as required for most CNNs\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(), # Augmentation to prevent overfitting\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Use the paths confirmed by your os.walk loop\n",
        "train_dataset = datasets.ImageFolder('chest_xray/train', transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Validation set as required by the \"Validation Approach\" criteria\n",
        "val_dataset = datasets.ImageFolder('chest_xray/val', transform=transform)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "STqoHg2CMafp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class MedicalCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MedicalCNN, self).__init__()\n",
        "        # Soundness and originality of architecture\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            nn.Conv2d(16, 32, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2)\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(64 * 28 * 28, 128), nn.ReLU(),\n",
        "            nn.Dropout(0.5), # Prevents overfitting\n",
        "            nn.Linear(128, 1), nn.Sigmoid() # Binary output for Normal vs Pneumonia\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.classifier(self.features(x))\n",
        "\n",
        "model = MedicalCNN()"
      ],
      "metadata": {
        "id": "-grGMu9jMghC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001) # Hyperparameter choice\n",
        "criterion = nn.BCELoss() # Loss computation\n",
        "\n",
        "for epoch in range(15):\n",
        "    model.train()\n",
        "    for images, labels in train_loader:\n",
        "        # 1. Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels.float().unsqueeze(1))\n",
        "\n",
        "        # 2. Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # 3. Optimizer update\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch+1} complete.\")"
      ],
      "metadata": {
        "id": "So2oCuatMjJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Define the file name based on your team name as per submission conventions\n",
        "model_save_path = \"pneumoniaDetectorModelv1.pth\"\n",
        "\n",
        "# Save the model state_dict (weights)\n",
        "torch.save(model.state_dict(), model_save_path)\n",
        "\n",
        "print(f\"Model successfully saved to {model_save_path}\")"
      ],
      "metadata": {
        "id": "Qa85rfGMW7FW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install scikit-learn seaborn matplotlib"
      ],
      "metadata": {
        "id": "eZn9aOkVW-Nl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Preprocessing for testing (must match training resize/normalization) [cite: 89, 161]\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Loading the test set [cite: 126]\n",
        "# Note: Use the path confirmed in your directory: 'chest_xray/test'\n",
        "test_dataset = datasets.ImageFolder('chest_xray/test', transform=test_transform)\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "print(f\"Test loader defined with {len(test_dataset)} images.\")"
      ],
      "metadata": {
        "id": "uOPSlh7IXBae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "model.eval() # Set model to evaluation mode\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad(): # Disable gradient calculation for testing\n",
        "    for images, labels in test_loader:\n",
        "        outputs = model(images)\n",
        "        # Convert probabilities to binary predictions (0 or 1)\n",
        "        preds = (outputs > 0.5).float()\n",
        "\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Calculate Accuracy\n",
        "accuracy = accuracy_score(all_labels, all_preds)\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Detailed Classification Report (Precision, Recall, F1-Score)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(all_labels, all_preds, target_names=['Normal', 'Pneumonia']))"
      ],
      "metadata": {
        "id": "rgXtoxqKXEne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "model.eval() # Set to evaluation mode (freezes BatchNorm and Dropout) [cite: 125, 220]\n",
        "y_true, y_pred = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        outputs = model(images)\n",
        "        predictions = (outputs > 0.5).float()\n",
        "\n",
        "        y_true.extend(labels.tolist())\n",
        "        y_pred.extend(predictions.reshape(-1).tolist())\n",
        "\n",
        "# 1. Primary & Secondary Metrics [cite: 135, 136]\n",
        "print(f\"Test Accuracy: {accuracy_score(y_true, y_pred) * 100:.2f}%\")\n",
        "print(\"\\nDetailed Report:\\n\", classification_report(y_true, y_pred, target_names=['Normal', 'Pneumonia']))\n",
        "\n",
        "# 2. Mandatory Confusion Matrix [cite: 137]\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Greens',\n",
        "            xticklabels=['Normal', 'Pneumonia'],\n",
        "            yticklabels=['Normal', 'Pneumonia'])\n",
        "plt.title('Confusion Matrix - Pneumonia Detection Model')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YmHhUOIyXX_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Training transforms with Augmentation\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Test transforms (No Augmentation)\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "train_loader = DataLoader(datasets.ImageFolder('chest_xray/train', transform=train_transform), batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(datasets.ImageFolder('chest_xray/test', transform=test_transform), batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "sEhJJbs1YODy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Training transforms with Augmentation\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Test transforms (No Augmentation)\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "train_loader = DataLoader(datasets.ImageFolder('chest_xray/train', transform=train_transform), batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(datasets.ImageFolder('chest_xray/test', transform=test_transform), batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "3OSaVi28Y0Mc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class MedicalCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MedicalCNN, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, 3, padding=1), nn.BatchNorm2d(16), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            nn.Conv2d(16, 32, 3, padding=1), nn.BatchNorm2d(32), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            nn.Conv2d(32, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(2)\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(64 * 28 * 28, 128), nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(128, 1) # Outputting logits for BCEWithLogitsLoss\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.classifier(self.features(x))\n",
        "\n",
        "model = MedicalCNN()"
      ],
      "metadata": {
        "id": "C48rM24sZRFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# pos_weight < 1 makes the model more 'conservative' about predicting Pneumonia\n",
        "# This specifically targets your low Normal recall\n",
        "criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([0.7]).to(device))\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Scheduler reduces LR by half if validation loss doesn't improve for 3 epochs\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.5)\n",
        "\n",
        "for epoch in range(25): # Increased epochs for stabilization\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels.float().unsqueeze(1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    # Placeholder for validation loss check to step the scheduler\n",
        "    # scheduler.step(val_loss)\n",
        "    print(f\"Epoch {epoch+1} | Loss: {train_loss/len(train_loader):.4f}\")"
      ],
      "metadata": {
        "id": "wo_GaVEAZrrX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Define the file name based on your team name as per submission conventions\n",
        "model_save_path = \"pneumoniaDetectorModelv2.pth\"\n",
        "\n",
        "# Save the model state_dict (weights)\n",
        "torch.save(model.state_dict(), model_save_path)\n",
        "\n",
        "print(f\"Model successfully saved to {model_save_path}\")"
      ],
      "metadata": {
        "id": "Hg3xeGW9pHQD"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "datasetId": 17810,
          "sourceId": 23812,
          "sourceType": "datasetVersion"
        }
      ],
      "isGpuEnabled": false,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}